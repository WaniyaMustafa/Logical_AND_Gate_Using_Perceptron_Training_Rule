# Logical_AND_Gate_Using_Perceptron_Training_Rule
***ReadMe***  
Logical AND Gate Implemented in Python from Scratch using Single Layer Perceptron.  
![image](https://user-images.githubusercontent.com/91070351/177524544-ef0c773d-ada2-4047-9ba6-510de5017625.png)  

I have used Perceptron Training Rule to tune my weights and bias.  
***Perceptron Rule***  
So the Formula for Tuning the weights for Perceptron Training Rule is:  
![image](https://user-images.githubusercontent.com/91070351/177520980-7a028c92-6fab-4895-9737-7c76ee780785.png)  
***Training Data***  
Here Are My Training Dataset Examples/Points.  
![image](https://user-images.githubusercontent.com/91070351/177520181-3ff6cce0-f9e3-416a-b34f-55d6aa94b729.png)
  
***Setup***  
Just Download the trainANDModel.py and PredictAND.py from here or clone the Repo in any Directory.  
Open Editor of your Choice or Open the Terminal and run "python trainANDModel.py" You will se the Output as:  
![image](https://user-images.githubusercontent.com/91070351/177522832-2663f05d-3de5-425e-8311-b4d466a9afc1.png)

It will create a trained Model Pickle file Named "AND.pkl" in the Same Directory.  
Now Run the File PredictAND.py Type your Inputs and Boomb Model Will predict Output. Sample:  
![image](https://user-images.githubusercontent.com/91070351/177523367-07a9f339-3e67-4d6d-9a0b-75405b7c9ec4.png)  
***Having Issues??***  
just hit me up if you have any Problem or confusion in the Code or   
Mathematical Aspects of the Model. Otherwise it is pretty straight Forward.  

